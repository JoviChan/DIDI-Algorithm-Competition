{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dateutil.parser import parse as dateutil_parse\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import StandardScaler as skStandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.cross_validation import train_test_split,cross_val_score\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "from mylib import myStandardScaler,process_order,process_traffic,get_order_group,get_traffic_group,XY_order_traffic, prediction2submit, \\\n",
    "    Search_Model, DISTRICTS, request_answer_count, XY_order_cluster\n",
    "\n",
    "PATH = 'season_1/'\n",
    "CLEAN_PATH = PATH+'clean/'\n",
    "SEARCH_PATH = 'clf_rf/'\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_map = pd.read_csv(CLEAN_PATH+'cluster_map.csv',index_col=0)\n",
    "poi = pd.read_csv(CLEAN_PATH+'poi.csv',index_col=0)\n",
    "train_order_group = pd.read_pickle(CLEAN_PATH+'train_order_group.pickle')\n",
    "test_order_group = pd.read_pickle(CLEAN_PATH+'test_order_group.pickle')\n",
    "train_traffic_group = pd.read_pickle(CLEAN_PATH+'train_traffic_group.pickle')\n",
    "test_traffic_group = pd.read_pickle(CLEAN_PATH+'test_traffic_group.pickle')\n",
    "test_target = pd.read_csv(CLEAN_PATH+'test_target.csv',index_col=0,parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_traffic_group.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ra_count = request_answer_count(train_order_group)\n",
    "train_cluster = pd.concat([poi, ra_count], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_range = 8\n",
    "clf = KMeans(n_clusters=label_range, max_iter=100000)\n",
    "s = clf.fit(train_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "district_label = pd.DataFrame({'label':clf.labels_}, index = train_cluster.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_slot = pd.Index(sorted(train_order_group.values()[0].index.unique()))\n",
    "train_slot = pd.Index(filter(lambda x: x%1000 >4,train_slot))\n",
    "test_slot = test_target['datetimeslot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take 00:00:15\n"
     ]
    }
   ],
   "source": [
    "now = time.time()\n",
    "\n",
    "train_XY_group = dict()\n",
    "for label in range(label_range):\n",
    "    train_XY_group[label] = XY_order_cluster(label, district_label,train_order_group,train_traffic_group,train_slot)\n",
    "    train_XY_group[label][0] = train_XY_group[label][0].sort_index(axis=1)\n",
    "test_XY_group = dict()\n",
    "for district in DISTRICTS:\n",
    "    test_XY_group[district] = XY_order_traffic(district,test_order_group,test_traffic_group,test_slot)\n",
    "    test_XY_group[district][0] = test_XY_group[district][0].sort_index(axis=1)\n",
    "for label in range(label_range):\n",
    "    scaler = myStandardScaler()\n",
    "    train_XY_group[label][0] = scaler.fit_transform(train_XY_group[label][0])\n",
    "    for district in DISTRICTS:\n",
    "        if(district_label.ix[district].values[0] == label and district in train_traffic_group):\n",
    "            Xnumerical = test_XY_group[district][0][test_XY_group[district][0].columns[test_XY_group[district][0].dtypes != bool]]\n",
    "            test_XY_group[district][0] = scaler.transform(test_XY_group[district][0])    \n",
    "stop = time.time()\n",
    "print 'Take %02d:%02d:%02d' % ((stop-now)/3600,(stop-now)/60,(stop-now)%60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 单独处理没有交通信息的那个地点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for district in DISTRICTS:\n",
    "    if district not in train_traffic_group:\n",
    "        train_XY_group[district] = XY_order_traffic(district,train_order_group,train_traffic_group,train_slot)\n",
    "        scaler = myStandardScaler()\n",
    "        train_XY_group[district][0] = scaler.fit_transform(train_XY_group[district][0])\n",
    "        test_XY_group[district][0] = scaler.transform(test_XY_group[district][0])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 'c4ec24e0a58ebedaa1661e5c09e47bb5']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_XY_group.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster + Searcher + Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching cluster 0...\n",
      "Best Params: {'warm_start': False, 'oob_score': False, 'n_jobs': 1, 'verbose': 0, 'max_leaf_nodes': None, 'bootstrap': True, 'min_samples_leaf': 6, 'n_estimators': 80, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'criterion': 'mse', 'random_state': None, 'max_features': None, 'max_depth': 12}\n",
      "Fit score: 0.668004283328\n",
      "The metrics: 0.585805330665\n",
      "Take 00:12:04\n",
      "Searching cluster 1...\n",
      "Best Params: {'warm_start': False, 'oob_score': False, 'n_jobs': 1, 'verbose': 0, 'max_leaf_nodes': None, 'bootstrap': True, 'min_samples_leaf': 6, 'n_estimators': 80, 'min_samples_split': 6, 'min_weight_fraction_leaf': 0.0, 'criterion': 'mse', 'random_state': None, 'max_features': None, 'max_depth': 11}\n",
      "Fit score: 0.835710664019\n",
      "The metrics: 0.221027114368\n",
      "Take 06:388:07\n",
      "Searching cluster 2...\n",
      "Best Params: {'warm_start': False, 'oob_score': False, 'n_jobs': 1, 'verbose': 0, 'max_leaf_nodes': None, 'bootstrap': True, 'min_samples_leaf': 2, 'n_estimators': 80, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'criterion': 'mse', 'random_state': None, 'max_features': None, 'max_depth': 17}\n",
      "Fit score: 0.978772782413\n",
      "The metrics: 0.397823353825\n",
      "Take 00:09:30\n",
      "Searching cluster 3...\n",
      "Best Params: {'warm_start': False, 'oob_score': False, 'n_jobs': 1, 'verbose': 0, 'max_leaf_nodes': None, 'bootstrap': True, 'min_samples_leaf': 6, 'n_estimators': 80, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'criterion': 'mse', 'random_state': None, 'max_features': None, 'max_depth': 17}\n",
      "Fit score: 0.948210088946\n",
      "The metrics: 0.658779224528\n",
      "Take 00:17:06\n",
      "Searching cluster 4...\n",
      "Best Params: {'warm_start': False, 'oob_score': False, 'n_jobs': 1, 'verbose': 0, 'max_leaf_nodes': None, 'bootstrap': True, 'min_samples_leaf': 6, 'n_estimators': 80, 'min_samples_split': 6, 'min_weight_fraction_leaf': 0.0, 'criterion': 'mse', 'random_state': None, 'max_features': None, 'max_depth': 17}\n",
      "Fit score: 0.935269068532\n",
      "The metrics: 0.578679186692\n",
      "Take 00:52:23\n",
      "Searching cluster 5...\n",
      "Best Params: {'warm_start': False, 'oob_score': False, 'n_jobs': 1, 'verbose': 0, 'max_leaf_nodes': None, 'bootstrap': True, 'min_samples_leaf': 2, 'n_estimators': 80, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'criterion': 'mse', 'random_state': None, 'max_features': None, 'max_depth': 17}\n",
      "Fit score: 0.96531177487\n",
      "The metrics: 0.545683082679\n",
      "Take 00:07:37\n",
      "Searching cluster 6...\n",
      "Best Params: {'warm_start': False, 'oob_score': False, 'n_jobs': 1, 'verbose': 0, 'max_leaf_nodes': None, 'bootstrap': True, 'min_samples_leaf': 6, 'n_estimators': 80, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'criterion': 'mse', 'random_state': None, 'max_features': None, 'max_depth': 17}\n",
      "Fit score: 0.928701339268\n",
      "The metrics: 0.623604904163\n",
      "Take 00:07:36\n",
      "Searching cluster 7...\n",
      "Best Params: {'warm_start': False, 'oob_score': False, 'n_jobs': 1, 'verbose': 0, 'max_leaf_nodes': None, 'bootstrap': True, 'min_samples_leaf': 2, 'n_estimators': 80, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'criterion': 'mse', 'random_state': None, 'max_features': None, 'max_depth': 16}\n",
      "Fit score: 0.964599186216\n",
      "The metrics: 0.66673326106\n",
      "Take 00:24:20\n",
      "We can together! :)\n"
     ]
    }
   ],
   "source": [
    "grid_params = {'n_estimators': [80] ,'max_depth': np.arange(10, 18), 'min_samples_leaf': [2, 6], \n",
    "                     'min_samples_split': [2, 6], 'max_features': ['log2', 'sqrt',None]}\n",
    "search_models = {key: Search_Model(RandomForestRegressor) for key in train_XY_group.keys()}\n",
    "test_prediction = dict()\n",
    "for key, model in search_models.items():\n",
    "    if len(os.listdir(SEARCH_PATH))==66:\n",
    "        print 'We can together! :)'\n",
    "        break\n",
    "    if key in DISTRICTS:\n",
    "        now = time.time()\n",
    "        print 'Searching %s...'%key\n",
    "        model.fit(grid_params,*train_XY_group[key])\n",
    "        test_prediction[key] = model.predict(test_XY_group[key][0]) - test_XY_group[key][1].fillna(0)\n",
    "        with open(SEARCH_PATH+'test_prediction_%s.pickle'%(key),'wb') as f:\n",
    "            pickle.dump(test_prediction[key],f)\n",
    "        stop = time.time()\n",
    "        print 'Take %02d:%02d:%02d' % ((stop-now)/3600,(stop-now)/60,(stop-now)%60)\n",
    "    else:\n",
    "        now = time.time()\n",
    "        print 'Searching cluster %d...'%key\n",
    "        model.fit(grid_params,*train_XY_group[key])\n",
    "        for district in DISTRICTS:\n",
    "            if(district_label.ix[district].values[0] == key and district in train_traffic_group): \n",
    "                test_prediction[district] = model.predict(test_XY_group[district][0]) - test_XY_group[district][1].fillna(0)\n",
    "                with open(SEARCH_PATH + 'test_prediction_%s.pickle'%(district),'wb') as f:\n",
    "                    pickle.dump(test_prediction[district],f)\n",
    "        stop = time.time()\n",
    "        print 'Take %02d:%02d:%02d' % ((stop-now)/3600,(stop-now)/60,(stop-now)%60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for filePath in os.listdir(SEARCH_PATH):\n",
    "    name = filePath.split('.')[0]\n",
    "    district = name.split('_')[-1]\n",
    "    test_prediction[district] = pickle.load(open(SEARCH_PATH + filePath,\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit = prediction2submit(test_prediction, cluster_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ceil_gap(x):\n",
    "    if(x < 1):\n",
    "        res = 1\n",
    "    else:\n",
    "        res = x\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newgap = submit['prediction'].apply(lambda x: ceil_gap(x))\n",
    "submit['gap'] = newgap\n",
    "submit = submit.drop('prediction', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit.to_csv(PATH+'submit/cluster8_searchrf_order_traffic.csv',index=None,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

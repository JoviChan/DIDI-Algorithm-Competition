{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dateutil.parser import parse as dateutil_parse\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import graphlab as gl\n",
    "from graphlab import SFrame\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import StandardScaler as skStandardScaler\n",
    "from sklearn.cross_validation import train_test_split,cross_val_score\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "from mylib import myStandardScaler,process_order,process_traffic,get_order_group,get_traffic_group,XY_order_traffic, prediction2submit, \\\n",
    "    Search_Model, DISTRICTS, request_answer_count, XY_order_cluster, mymetrics\n",
    "\n",
    "PATH = 'season_1/'\n",
    "CLEAN_PATH = PATH+'clean/'\n",
    "SEARCH_PATH = 'clf_gl/'\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_map = pd.read_csv(CLEAN_PATH+'cluster_map.csv',index_col=0)\n",
    "poi = pd.read_csv(CLEAN_PATH+'poi.csv',index_col=0)\n",
    "train_order_group = pd.read_pickle(CLEAN_PATH+'train_order_group.pickle')\n",
    "test_order_group = pd.read_pickle(CLEAN_PATH+'test_order_group.pickle')\n",
    "train_traffic_group = pd.read_pickle(CLEAN_PATH+'train_traffic_group.pickle')\n",
    "test_traffic_group = pd.read_pickle(CLEAN_PATH+'test_traffic_group.pickle')\n",
    "test_target = pd.read_csv(CLEAN_PATH+'test_target.csv',index_col=0,parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ra_count = request_answer_count(train_order_group)\n",
    "train_cluster = pd.concat([poi, ra_count], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_range = 12\n",
    "clf = KMeans(n_clusters=label_range, max_iter=100000)\n",
    "s = clf.fit(train_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  3,  3,  4,  3,  3,  3,  3,  3,  4, 11,  3, 11, 11,  7,  3,  3,\n",
       "        3,  1,  3,  3,  3,  3,  3,  7,  3,  3,  3,  8,  3,  3,  9,  3,  3,\n",
       "        3,  3,  1,  3,  7,  3,  4, 10,  6,  7,  3,  3,  5,  7,  7,  3,  7,\n",
       "        7,  3,  0,  3,  3,  2,  3,  3,  7,  3,  0,  3,  3,  0,  3])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "district_label = pd.DataFrame({'label':clf.labels_}, index = train_cluster.index)\n",
    "clf.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_slot = pd.Index(sorted(train_order_group.values()[0].index.unique()))\n",
    "train_slot = pd.Index(filter(lambda x: x%1000 >4,train_slot))\n",
    "test_slot = test_target['datetimeslot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take 00:00:09\n"
     ]
    }
   ],
   "source": [
    "now = time.time()\n",
    "\n",
    "train_XY_group = dict()\n",
    "for label in range(label_range):\n",
    "    train_XY_group[label] = XY_order_cluster(label, district_label,train_order_group,train_traffic_group,train_slot)\n",
    "    train_XY_group[label][0] = train_XY_group[label][0].sort_index(axis=1)\n",
    "test_XY_group = dict()\n",
    "for district in DISTRICTS:\n",
    "    test_XY_group[district] = XY_order_traffic(district,test_order_group,test_traffic_group,test_slot)\n",
    "    test_XY_group[district][0] = test_XY_group[district][0].sort_index(axis=1)\n",
    "for label in range(label_range):\n",
    "    scaler = myStandardScaler()\n",
    "    train_XY_group[label][0] = scaler.fit_transform(train_XY_group[label][0])\n",
    "    for district in DISTRICTS:\n",
    "        if(district_label.ix[district].values[0] == label and district in train_traffic_group):\n",
    "            Xnumerical = test_XY_group[district][0][test_XY_group[district][0].columns[test_XY_group[district][0].dtypes != bool]]\n",
    "            test_XY_group[district][0] = scaler.transform(test_XY_group[district][0])    \n",
    "stop = time.time()\n",
    "print 'Take %02d:%02d:%02d' % ((stop-now)/3600,(stop-now)/60,(stop-now)%60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 单独处理没有交通信息的那个地点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for district in DISTRICTS:\n",
    "    if district not in train_traffic_group:\n",
    "        train_XY_group[district] = XY_order_traffic(district,train_order_group,train_traffic_group,train_slot)\n",
    "        scaler = myStandardScaler()\n",
    "        train_XY_group[district][0] = scaler.fit_transform(train_XY_group[district][0])\n",
    "        test_XY_group[district][0] = scaler.transform(test_XY_group[district][0])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 'c4ec24e0a58ebedaa1661e5c09e47bb5']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_XY_group.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching 0...\n",
      "The metrics: 0.469801871863\n",
      "Take 00:01:49\n",
      "Searching 1...\n",
      "The metrics: 0.148137409084\n",
      "Take 00:01:22\n",
      "Searching 2...\n",
      "The metrics: 0.073780754724\n",
      "Take 00:00:45\n",
      "Searching 3...\n",
      "The metrics: 1.29889032318\n",
      "Take 00:45:51\n",
      "Searching 4...\n",
      "The metrics: 0.27095566053\n",
      "Take 00:01:50\n",
      "Searching 5...\n",
      "The metrics: 0.228791366806\n",
      "Take 00:00:40\n",
      "Searching 6...\n",
      "The metrics: 0.132574374631\n",
      "Take 00:00:40\n",
      "Searching 7...\n",
      "The metrics: 0.660470792026\n",
      "Take 00:04:46\n",
      "Searching 8...\n",
      "The metrics: 0.125653856081\n",
      "Take 00:00:43\n",
      "Searching 9...\n",
      "The metrics: 0.257804671502\n",
      "Take 00:00:44\n",
      "Searching 10...\n",
      "The metrics: 0.397719063981\n",
      "Take 00:00:42\n",
      "Searching 11...\n",
      "The metrics: 0.385025134996\n",
      "Take 00:02:07\n",
      "Searching c4ec24e0a58ebedaa1661e5c09e47bb5...\n",
      "The metrics: 1.07140233709\n",
      "Take 00:00:44\n",
      "Totally take 01:62:49\n"
     ]
    }
   ],
   "source": [
    "all_now = time.time()\n",
    "test_prediction = dict()\n",
    "for key in train_XY_group.keys():\n",
    "    if key in DISTRICTS:\n",
    "        now = time.time()\n",
    "        print 'Searching %s...'%key\n",
    "        trainSet = SFrame(train_XY_group[key][0])\n",
    "        targetSet = SFrame(train_XY_group[key][1])\n",
    "        trainSet = trainSet.add_columns(targetSet)\n",
    "        model = gl.boosted_trees_regression.create(trainSet, target='X1', features=train_XY_group[key][0].columns,\n",
    "                                          max_iterations=5000, validation_set='auto', max_depth=5, step_size=0.03, \n",
    "                                          min_loss_reduction=1.0, min_child_weight=0.1, row_subsample=1.0, column_subsample=1.0, verbose=False, \n",
    "                                          random_seed=1, metric='auto')\n",
    "        sPredict = model.predict(SFrame(test_XY_group[key][0]))\n",
    "        myPredict = pd.Series(sPredict.to_numpy(), index = test_XY_group[key][0].index)\n",
    "        test_prediction[key] = myPredict - test_XY_group[key][1].fillna(0)\n",
    "        with open(SEARCH_PATH+'test_prediction_%s.pickle'%(key),'wb') as f:\n",
    "            pickle.dump(test_prediction[key],f)\n",
    "        print \"The metrics:\", mymetrics(pd.Series(model.predict(SFrame(train_XY_group[key][0])).to_numpy()), pd.Series(trainSet['X1'].to_numpy()))\n",
    "        stop = time.time()\n",
    "        print 'Take %02d:%02d:%02d' % ((stop-now)/3600,(stop-now)/60,(stop-now)%60)\n",
    "    else:\n",
    "        now = time.time()\n",
    "        print 'Searching %d...'%key\n",
    "        trainSet = SFrame(train_XY_group[key][0])\n",
    "        targetSet = SFrame(train_XY_group[key][1])\n",
    "        trainSet = trainSet.add_columns(targetSet)\n",
    "        model = gl.boosted_trees_regression.create(trainSet, target='X1', features=train_XY_group[key][0].columns,\n",
    "                                          max_iterations=5000, validation_set='auto', max_depth=5, step_size=0.03, \n",
    "                                          min_loss_reduction=1.0, min_child_weight=0.1, row_subsample=1.0, column_subsample=1.0, verbose=False, \n",
    "                                          random_seed=1, metric='auto')\n",
    "        for district in DISTRICTS:\n",
    "            if(district_label.ix[district].values[0] == key and district in train_traffic_group): \n",
    "                sPredict = model.predict(SFrame(test_XY_group[district][0]))\n",
    "                myPredict = pd.Series(sPredict.to_numpy(), index = test_XY_group[district][0].index)\n",
    "                test_prediction[district] = myPredict - test_XY_group[district][1].fillna(0)\n",
    "                with open(SEARCH_PATH+'test_prediction_%s.pickle'%(district),'wb') as f:\n",
    "                    pickle.dump(test_prediction[district],f)\n",
    "        print \"The metrics:\", mymetrics(pd.Series(model.predict(SFrame(train_XY_group[key][0])).to_numpy()), \\\n",
    "                                                pd.Series(trainSet['X1'].to_numpy()))\n",
    "        stop = time.time()\n",
    "        print 'Take %02d:%02d:%02d' % ((stop-now)/3600,(stop-now)/60,(stop-now)%60)\n",
    "all_stop = time.time()\n",
    "print 'Totally take %02d:%02d:%02d' % ((all_stop-all_now)/3600,(all_stop-all_now)/60,(all_stop-all_now)%60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit = prediction2submit(test_prediction, cluster_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ceil_gap(x):\n",
    "    if(x < 1):\n",
    "        res = 1\n",
    "    else:\n",
    "        res = x\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newgap = submit['prediction'].apply(lambda x: ceil_gap(x))\n",
    "submit['gap'] = newgap\n",
    "submit = submit.drop('prediction', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit.to_csv(PATH+'submit/cluster_rl_boosted_tree_12.csv',index=None,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
